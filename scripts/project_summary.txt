--- .\main.py ---
# main.py
import os
import sys
import argparse
import uvicorn
from dotenv import load_dotenv
from quant_system.utils import logger, set_log_level, DEBUG, INFO, ErrorHandler

# Load environment variables from .env file
logger.info("Loading environment variables from .env file")
load_dotenv()

def start_api(host="0.0.0.0", port=8000, reload=False):
    """Start the FastAPI server"""
    logger.info(f"Starting API server on {host}:{port} (reload={reload})")
    try:
        uvicorn.run("quant_system.interface.api:app", host=host, port=port, reload=reload)
    except Exception as e:
        logger.error(f"Failed to start API server: {e}")
        raise

def start_cli(debug=False):
    """Start the command-line interface"""
    logger.info("Starting CLI interface")
    try:
        from quant_system.interface.cli import main
        
        # If debug mode is enabled in the main script, ensure it's passed to CLI
        if debug and '--debug' not in sys.argv:
            sys.argv.append('--debug')
            
        main()
    except Exception as e:
        logger.error(f"Failed to start CLI interface: {e}")
        raise

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Quant System")
    subparsers = parser.add_subparsers(dest="mode", help="Run mode")
    
    # API mode
    api_parser = subparsers.add_parser("api", help="Start the web API")
    api_parser.add_argument("--host", default="0.0.0.0", help="API host")
    api_parser.add_argument("--port", "-p", type=int, default=8000, help="API port")
    api_parser.add_argument("--reload", "-r", action="store_true", help="Enable auto-reload")
    
    # CLI mode - forward all remaining arguments to the CLI
    cli_parser = subparsers.add_parser("cli", help="Run command-line interface")
    
    # Add common arguments
    parser.add_argument("--debug", action="store_true", help="Enable debug logging")
    
    args, remaining = parser.parse_known_args()
    
    # Set logging level based on arguments
    if hasattr(args, 'debug') and args.debug:
        set_log_level(DEBUG)
        logger.debug("Debug logging enabled")
    
    logger.info(f"Starting Quant System in {args.mode or 'help'} mode")
    
    with ErrorHandler(context="main execution", exit_on_error=True):
        if args.mode == "api":
            start_api(args.host, args.port, args.reload)
        elif args.mode == "cli":
            # Pass remaining arguments to CLI
            sys.argv = [sys.argv[0]] + remaining
            start_cli(debug=args.debug)
        else:
            logger.info("No mode specified, showing help")
            parser.print_help()


--- .\quant_system\utils.py ---
import os
import sys
import logging
import traceback
from datetime import datetime
from pathlib import Path

# Configure logging levels
DEBUG = logging.DEBUG
INFO = logging.INFO
WARNING = logging.WARNING
ERROR = logging.ERROR
CRITICAL = logging.CRITICAL

# Create logs directory if it doesn't exist
logs_dir = Path("logs")
logs_dir.mkdir(exist_ok=True)

# Create a timestamp for log files
timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
log_file = logs_dir / f"quant_system_{timestamp}.log"

# Configure root logger
logger = logging.getLogger("quant_system")
logger.setLevel(logging.DEBUG)

# Create file handler for logging to file
file_handler = logging.FileHandler(log_file)
file_handler.setLevel(logging.DEBUG)

# Create console handler for logging to console
console_handler = logging.StreamHandler(sys.stdout)
console_handler.setLevel(logging.INFO)

# Create formatters
file_formatter = logging.Formatter(
    "[%(asctime)s] %(levelname)s [%(name)s.%(funcName)s:%(lineno)d] %(message)s"
)
console_formatter = logging.Formatter(
    "[%(levelname)s] %(message)s"
)

# Apply formatters to handlers
file_handler.setFormatter(file_formatter)
console_handler.setFormatter(console_formatter)

# Add handlers to logger
logger.addHandler(file_handler)
logger.addHandler(console_handler)

# Function to log exceptions with traceback
def log_exception(exc_info=None):
    """
    Log an exception with full traceback.
    
    Args:
        exc_info: Exception info from sys.exc_info(). If None, current exception is used.
    """
    if exc_info is None:
        exc_info = sys.exc_info()
    
    if exc_info[0] is not None:
        exc_type, exc_value, exc_tb = exc_info
        tb_str = "".join(traceback.format_exception(exc_type, exc_value, exc_tb))
        logger.error(f"Exception occurred:\n{tb_str}")

# Context manager for error handling
class ErrorHandler:
    def __init__(self, context="operation", exit_on_error=False):
        """
        Context manager for handling and logging errors.
        
        Args:
            context: Description of the operation being performed
            exit_on_error: Whether to exit the program on error
        """
        self.context = context
        self.exit_on_error = exit_on_error
    
    def __enter__(self):
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        if exc_type is not None:
            logger.error(f"Error during {self.context}: {exc_val}")
            log_exception((exc_type, exc_val, exc_tb))
            
            if self.exit_on_error:
                logger.critical(f"Exiting due to error in {self.context}")
                sys.exit(1)
            
            return True  # Suppress the exception
        return False

# Function to set logging level
def set_log_level(level):
    """
    Set the logging level for the console handler.
    
    Args:
        level: Logging level (DEBUG, INFO, WARNING, ERROR, CRITICAL)
    """
    console_handler.setLevel(level)
    logger.info(f"Console log level set to {logging.getLevelName(level)}")

# Function to get a named logger (child of root logger)
def get_logger(name):
    """
    Get a named logger that inherits from the root logger.
    
    Args:
        name: Name for the logger, typically the module name
        
    Returns:
        Logger instance
    """
    return logging.getLogger(f"quant_system.{name}") 

--- .\quant_system\analysis\backtest.py ---
# quant_system/analysis/backtest.py
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from typing import List, Tuple, Dict, Any

from quant_system.utils import get_logger

# Initialize logger
logger = get_logger("analysis.backtest")

class MarketBacktester:
    """Backtest performance following certain market conditions"""
    
    @staticmethod
    def find_similar_conditions(df, conditions, lookback_days=None, min_match_count=1, exact_match=False):
        """Find historical periods with similar conditions
        
        Args:
            df: DataFrame with price data and indicators
            conditions: List of market conditions to search for
            lookback_days: Number of days to look back (None for all available data)
            min_match_count: Minimum number of matching conditions required
            exact_match: Whether all conditions must match exactly
            
        Returns:
            List of tuples with (date, matched_conditions)
        """
        if df.empty:
            logger.warning("Cannot find similar conditions: Empty dataframe")
            return []
        
        similar_dates = []
        condition_keywords = [cond.lower() for cond in conditions]
        
        # Determine lookback range
        if lookback_days is None:
            lookback_df = df.copy()
        else:
            # Don't look back more than the available data
            max_lookback = min(lookback_days, len(df) - 1)
            lookback_df = df.iloc[:-1].iloc[-max_lookback:]
        
        logger.info(f"Searching for similar conditions across {len(lookback_df)} historical data points")
        logger.debug(f"Target conditions: {', '.join(conditions)}")
        
        # Helper function to check if strings match based on keywords
        def conditions_match(target, reference):
            """Check if a condition matches another based on keywords"""
            target_lower = target.lower()
            reference_lower = reference.lower()
            
            # Check for exact matches first
            if target_lower == reference_lower:
                return True
                
            # Check for partial matches (one is substring of the other)
            if target_lower in reference_lower or reference_lower in target_lower:
                return True
                
            # Look for keyword matches
            target_words = set(target_lower.split())
            reference_words = set(reference_lower.split())
            
            # If they share at least 3 significant words, consider it a match
            shared_words = target_words.intersection(reference_words)
            significant_words = [w for w in shared_words if len(w) > 3 and w not in {'with', 'from', 'that', 'this', 'above', 'below', 'near'}]
            
            return len(significant_words) >= 2
        
        # Process each historical data point
        for i in range(len(lookback_df)):
            date = lookback_df.index[i]
            current = lookback_df.iloc[i]
            prev = lookback_df.iloc[i-1] if i > 0 else None
            
            # Skip rows with NaN values in key columns
            if pd.isna(current['close']):
                continue
                
            # Identify conditions at this historical point
            historical_conditions = MarketBacktester._identify_historical_conditions(lookback_df, i, prev)
            
            # Match conditions between current target and historical point
            matched_conditions = []
            
            for hist_cond in historical_conditions:
                # For each current condition we're looking for
                for target_cond in conditions:
                    if conditions_match(target_cond, hist_cond):
                        matched_conditions.append(hist_cond)
                        break
            
            # Check if we have enough matches
            if len(matched_conditions) >= min_match_count:
                if exact_match and len(matched_conditions) < len(conditions):
                    # Skip if exact match required but not all conditions matched
                    continue
                    
                similar_dates.append((date, matched_conditions))
                logger.debug(f"Found match at {date.date()}: {', '.join(matched_conditions)}")
        
        logger.info(f"Found {len(similar_dates)} similar historical instances")
        return similar_dates
    
    @staticmethod
    def _identify_historical_conditions(df, idx, prev):
        """Identify market conditions at a historical data point
        
        This is a simplified version of MarketStructureAnalyzer.identify_conditions
        that works on a specific row in a dataframe.
        
        Args:
            df: DataFrame with price data and indicators
            idx: Index of the row to analyze
            prev: Previous row for calculations requiring it
            
        Returns:
            List of identified conditions
        """
        conditions = []
        current = df.iloc[idx]
        
        # ---------- TREND CONDITIONS ----------
        
        # Moving Average relationships
        if 'sma_20' in df.columns and pd.notna(current['sma_20']):
            if current['close'] > current['sma_20']:
                conditions.append("Price above 20-day MA")
            else:
                conditions.append("Price below 20-day MA")
        
        if 'sma_50' in df.columns and pd.notna(current['sma_50']):
            if current['close'] > current['sma_50']:
                conditions.append("Price above 50-day MA")
            else:
                conditions.append("Price below 50-day MA")
        
        if 'sma_200' in df.columns and pd.notna(current['sma_200']):
            if current['close'] > current['sma_200']:
                conditions.append("Price above 200-day MA")
            else:
                conditions.append("Price below 200-day MA")
            
            # Full trend with 50-day MA
            if 'sma_50' in df.columns and pd.notna(current['sma_50']):
                if current['close'] > current['sma_50'] and current['sma_50'] > current['sma_200']:
                    conditions.append("Bullish trend")
                elif current['close'] < current['sma_50'] and current['sma_50'] < current['sma_200']:
                    conditions.append("Bearish trend")
            
            # Golden/Death cross
            if prev is not None and 'sma_50' in df.columns and 'sma_200' in df.columns:
                if pd.notna(prev['sma_50']) and pd.notna(prev['sma_200']):
                    if prev['sma_50'] < prev['sma_200'] and current['sma_50'] > current['sma_200']:
                        conditions.append("Golden cross")
                    elif prev['sma_50'] > prev['sma_200'] and current['sma_50'] < current['sma_200']:
                        conditions.append("Death cross")
        
        # ---------- MOMENTUM CONDITIONS ----------
        
        # RSI
        if 'rsi_14' in df.columns and pd.notna(current['rsi_14']):
            if current['rsi_14'] < 30:
                conditions.append("Oversold")
            elif current['rsi_14'] > 70:
                conditions.append("Overbought")
        
        # MACD
        if all(col in df.columns for col in ['macd', 'macd_signal']) and pd.notna(current['macd']) and pd.notna(current['macd_signal']):
            if current['macd'] > current['macd_signal']:
                conditions.append("MACD above signal")
            else:
                conditions.append("MACD below signal")
                
            if prev is not None and pd.notna(prev['macd']) and pd.notna(prev['macd_signal']):
                if prev['macd'] < prev['macd_signal'] and current['macd'] > current['macd_signal']:
                    conditions.append("MACD bullish crossover")
                elif prev['macd'] > prev['macd_signal'] and current['macd'] < current['macd_signal']:
                    conditions.append("MACD bearish crossover")
        
        # ---------- VOLATILITY CONDITIONS ----------
        
        # Bollinger Bands
        bb_columns = ['bb_upper', 'bb_lower']
        if all(col in df.columns for col in bb_columns) and all(pd.notna(current[col]) for col in bb_columns):
            if current['close'] < current['bb_lower']:
                conditions.append("Price below lower Bollinger Band")
            elif current['close'] > current['bb_upper']:
                conditions.append("Price above upper Bollinger Band")
        
        # ATR
        if 'atr_14' in df.columns and pd.notna(current['atr_14']) and current['close'] > 0:
            atr_pct = (current['atr_14'] / current['close']) * 100
            
            if atr_pct < 1.5:
                conditions.append("Low volatility")
            elif atr_pct > 5.0:
                conditions.append("High volatility")
        
        # ---------- PRICE ACTION ----------
        
        # Recent price changes can be calculated even with minimal data
        lookback = min(5, idx)
        if lookback > 0 and pd.notna(df.iloc[idx-lookback]['close']):
            price_change = ((current['close'] / df.iloc[idx-lookback]['close']) - 1) * 100
            
            if price_change > 10:
                conditions.append("Strong bullish move")
            elif price_change < -10:
                conditions.append("Strong bearish move")
        
        return conditions
    
    @staticmethod
    def analyze_forward_returns(df, similar_dates, forward_days=[1, 5, 10, 20]):
        """Analyze returns following similar conditions
        
        Args:
            df: DataFrame with price data
            similar_dates: List of tuples with (date, matched_conditions)
            forward_days: List of forward periods to analyze
            
        Returns:
            (results_df, stats) - DataFrame with detailed results and dictionary with stats
        """
        if not similar_dates:
            logger.warning("No similar dates provided for forward returns analysis")
            return pd.DataFrame(), {}
        
        results = []
        
        logger.info(f"Analyzing forward returns for {len(similar_dates)} instances")
        logger.debug(f"Forward periods: {forward_days} days")
        
        for date, conditions in similar_dates:
            try:
                date_loc = df.index.get_loc(date)
                
                # Skip if we don't have enough forward data
                max_forward = max(forward_days)
                if date_loc + max_forward >= len(df):
                    logger.debug(f"Skipping {date.date()}: insufficient forward data")
                    continue
                
                # Calculate forward returns
                base_price = df.iloc[date_loc]['close']
                returns = {}
                
                for days in forward_days:
                    # Ensure we have data at the forward date
                    if date_loc + days < len(df) and pd.notna(df.iloc[date_loc + days]['close']):
                        future_price = df.iloc[date_loc + days]['close']
                        pct_return = (future_price - base_price) / base_price * 100
                        returns[f"{days}d_return"] = pct_return
                    else:
                        logger.debug(f"Missing {days}-day forward data for {date.date()}")
                
                # Add record if we have at least one valid return
                if returns:
                    results.append({
                        'date': date,
                        'conditions': conditions,
                        **returns
                    })
            except Exception as e:
                logger.error(f"Error calculating forward returns for {date}: {e}")
        
        # Convert to DataFrame for easy analysis
        results_df = pd.DataFrame(results) if results else pd.DataFrame()
        
        # Calculate stats
        stats = {}
        if not results_df.empty:
            for days in forward_days:
                col = f"{days}d_return"
                if col in results_df.columns:
                    valid_returns = results_df[col].dropna()
                    
                    if not valid_returns.empty:
                        stats[f"{days}d_mean"] = valid_returns.mean()
                        stats[f"{days}d_median"] = valid_returns.median()
                        stats[f"{days}d_std"] = valid_returns.std()
                        stats[f"{days}d_min"] = valid_returns.min()
                        stats[f"{days}d_max"] = valid_returns.max()
                        stats[f"{days}d_positive_pct"] = (valid_returns > 0).mean() * 100
                        stats[f"{days}d_count"] = len(valid_returns)
                        
                        logger.info(f"{days}-day forward returns: mean={stats[f'{days}d_mean']:.2f}%, "
                                  f"win rate={stats[f'{days}d_positive_pct']:.2f}%")
        else:
            logger.warning("No valid forward returns calculated")
        
        return results_df, stats

--- .\quant_system\analysis\llm_interface.py ---
import json
import requests
import os

class LLMAnalyzer:
    """Interface with LLMs for market analysis"""
    
    def __init__(self, api_key=None):
        self.api_key = api_key or os.environ.get("LLM_API_KEY")
        
    def generate_market_summary(self, market_data, conditions, backtest_stats):
        """Generate a market summary using an LLM"""
        # For simplicity, we'll use a template for now
        # In production, this would call an LLM API
        
        # Create a simplified market summary template
        recent_price = market_data.iloc[-1]['close']
        price_change_1d = (recent_price - market_data.iloc[-2]['close']) / market_data.iloc[-2]['close'] * 100
        price_change_7d = (recent_price - market_data.iloc[-7]['close']) / market_data.iloc[-7]['close'] * 100
        
        summary = f"""
        Market Summary:
        - Current price: ${recent_price:.2f}
        - 24h change: {price_change_1d:.2f}%
        - 7d change: {price_change_7d:.2f}%

        Market Conditions:
        - {', '.join(conditions)}

        Historical Performance (similar conditions):
        """
        
        # Add backtest stats
        for period, value in backtest_stats.items():
            if 'mean' in period:
                days = period.split('d_')[0]
                summary += f"- {days}-day forward return (avg): {value:.2f}%\n"
            if 'positive_pct' in period:
                days = period.split('d_')[0]
                summary += f"- {days}-day win rate: {value:.2f}%\n"
        
        return summary
    
    def analyze_with_llm(self, prompt, max_tokens=500):
        """Send a prompt to LLM API and get a response"""
        # This is a placeholder for actual LLM API call
        # In a real implementation, you would call Claude, GPT-4, etc.
        
        if not self.api_key:
            return "No API key provided for LLM service"
        
        # Example implementation for Claude API
        # Would need to be adapted based on the specific LLM service used
        try:
            response = requests.post(
                "https://api.anthropic.com/v1/messages",
                headers={
                    "x-api-key": self.api_key,
                    "anthropic-version": "2023-06-01",
                    "content-type": "application/json"
                },
                json={
                    "model": "claude-3-haiku-20240307",
                    "max_tokens": max_tokens,
                    "messages": [
                        {"role": "user", "content": prompt}
                    ]
                }
            )
            
            if response.status_code == 200:
                return response.json()["content"][0]["text"]
            else:
                return f"Error: {response.status_code} - {response.text}"
        
        except Exception as e:
            return f"Error calling LLM API: {str(e)}"


--- .\quant_system\analysis\market_structure.py ---
# quant_system/analysis/market_structure.py
import numpy as np
import pandas as pd
from typing import List

from quant_system.utils import get_logger

# Initialize logger
logger = get_logger("analysis.market_structure")

class MarketStructureAnalyzer:
    """Analyze market structure and conditions"""
    
    @staticmethod
    def identify_conditions(df) -> List[str]:
        """Identify current market conditions based on indicators
        
        This method analyzes the technical indicators in the dataframe and
        returns a list of detected market conditions.
        
        Args:
            df: DataFrame with price data and technical indicators
            
        Returns:
            List of identified market conditions
        """
        if df.empty:
            logger.warning("Cannot identify conditions: Empty dataframe")
            return ["Error: Empty dataframe"]
            
        # Log the shape and available columns for debugging
        logger.debug(f"Identifying conditions from dataframe with {len(df)} rows")
        logger.debug(f"Available columns: {', '.join(df.columns)}")
        
        # Check we have enough data for analysis
        if len(df) < 20:
            logger.warning(f"Not enough data for analysis: {len(df)} rows")
            return [f"Insufficient data: Only {len(df)} candles (minimum 20 required)"]
            
        conditions = []
        
        # Get the most recent indicators
        latest = df.iloc[-1]
        prev = df.iloc[-2] if len(df) > 1 else None
        
        # Check for required columns and handle missing data
        required_columns = {
            'close': 'closing price',
            'high': 'high price',
            'low': 'low price',
            'volume': 'volume'
        }
        
        missing_columns = [f"{name} ({desc})" for name, desc in required_columns.items() 
                          if name not in df.columns]
        
        if missing_columns:
            logger.error(f"Missing required price columns: {', '.join(missing_columns)}")
            return [f"Missing essential data: {', '.join(missing_columns)}"]
        
        # Log latest price data
        logger.debug(f"Latest close: {latest['close']}, high: {latest['high']}, low: {latest['low']}")
        
        # ---------- TREND ANALYSIS ----------
        
        # Check for short-term moving averages (less dependent on long history)
        if 'sma_20' in df.columns and pd.notna(latest['sma_20']):
            if latest['close'] > latest['sma_20']:
                conditions.append("Price above 20-day MA (short-term bullish)")
            else:
                conditions.append("Price below 20-day MA (short-term bearish)")
        
        # Check for medium-term trend using 50-day MA if available
        if 'sma_50' in df.columns and pd.notna(latest['sma_50']):
            if latest['close'] > latest['sma_50']:
                conditions.append("Price above 50-day MA (medium-term bullish)")
            else:
                conditions.append("Price below 50-day MA (medium-term bearish)")
        
        # Use 200-day MA only if we have enough data
        if 'sma_200' in df.columns and pd.notna(latest['sma_200']):
            # Full trend analysis with 50 and 200 day MAs
            if 'sma_50' in df.columns and pd.notna(latest['sma_50']):
                if latest['close'] > latest['sma_50'] and latest['sma_50'] > latest['sma_200']:
                    conditions.append("Bullish trend (price above 50 & 200 MAs)")
                elif latest['close'] < latest['sma_50'] and latest['sma_50'] < latest['sma_200']:
                    conditions.append("Bearish trend (price below 50 & 200 MAs)")
            
            # Golden/Death cross detection (requires previous data point)
            if prev is not None and 'sma_50' in df.columns and 'sma_200' in df.columns:
                if pd.notna(prev['sma_50']) and pd.notna(prev['sma_200']):
                    if prev['sma_50'] < prev['sma_200'] and latest['sma_50'] > latest['sma_200']:
                        conditions.append("Golden cross (50 MA crossed above 200 MA)")
                    elif prev['sma_50'] > prev['sma_200'] and latest['sma_50'] < latest['sma_200']:
                        conditions.append("Death cross (50 MA crossed below 200 MA)")
        else:
            logger.debug("200-day MA not available or contains NaN values")
                    
        # ---------- MOMENTUM ANALYSIS ----------
        
        # RSI analysis
        if 'rsi_14' in df.columns and pd.notna(latest['rsi_14']):
            logger.debug(f"Latest RSI: {latest['rsi_14']}")
            
            if latest['rsi_14'] < 30:
                conditions.append("Oversold (RSI below 30)")
            elif latest['rsi_14'] > 70:
                conditions.append("Overbought (RSI above 70)")
            elif 30 <= latest['rsi_14'] < 40:
                conditions.append("Near oversold territory (RSI between 30-40)")
            elif 60 < latest['rsi_14'] <= 70:
                conditions.append("Near overbought territory (RSI between 60-70)")
        
        # MACD analysis
        if all(col in df.columns for col in ['macd', 'macd_signal']) and pd.notna(latest['macd']) and pd.notna(latest['macd_signal']):
            logger.debug(f"Latest MACD: {latest['macd']}, Signal: {latest['macd_signal']}")
            
            # Current MACD position
            if latest['macd'] > latest['macd_signal']:
                conditions.append("MACD above signal line (bullish)")
            else:
                conditions.append("MACD below signal line (bearish)")
                
            # MACD crossover (requires previous data point)
            if prev is not None and pd.notna(prev['macd']) and pd.notna(prev['macd_signal']):
                if prev['macd'] < prev['macd_signal'] and latest['macd'] > latest['macd_signal']:
                    conditions.append("MACD bullish crossover")
                elif prev['macd'] > prev['macd_signal'] and latest['macd'] < latest['macd_signal']:
                    conditions.append("MACD bearish crossover")
        
        # ---------- VOLATILITY ANALYSIS ----------
        
        # Bollinger Bands analysis
        bb_columns = ['bb_upper', 'bb_lower']
        if all(col in df.columns for col in bb_columns) and all(pd.notna(latest[col]) for col in bb_columns):
            logger.debug(f"Bollinger Bands - Upper: {latest['bb_upper']}, Lower: {latest['bb_lower']}")
            
            if latest['close'] < latest['bb_lower']:
                conditions.append("Price below lower Bollinger Band (potential reversal up)")
            elif latest['close'] > latest['bb_upper']:
                conditions.append("Price above upper Bollinger Band (potential reversal down)")
            
            # Calculate bandwidth if possible
            if 'sma_20' in df.columns and pd.notna(latest['sma_20']):
                bandwidth = (latest['bb_upper'] - latest['bb_lower']) / latest['sma_20']
                
                if bandwidth < 0.10:  # Threshold can be adjusted
                    conditions.append("Low volatility (narrow Bollinger Bands)")
                elif bandwidth > 0.40:  # Threshold can be adjusted
                    conditions.append("High volatility (wide Bollinger Bands)")
        
        # ATR for volatility regime
        if 'atr_14' in df.columns and pd.notna(latest['atr_14']):
            # Calculate ATR as percentage of price
            atr_pct = (latest['atr_14'] / latest['close']) * 100
            logger.debug(f"ATR: {latest['atr_14']}, ATR%: {atr_pct:.2f}%")
            
            if atr_pct < 1.5:
                conditions.append("Low volatility regime (ATR < 1.5% of price)")
            elif atr_pct > 5.0:
                conditions.append("High volatility regime (ATR > 5% of price)")
                
        # Explicit volatility calculation
        if 'volatility_20' in df.columns and pd.notna(latest['volatility_20']):
            logger.debug(f"20-day volatility: {latest['volatility_20']}")
            
            # Use percentiles of recent volatility to determine regime
            if len(df) >= 60:  # Need at least some history to calculate percentiles
                volatility_series = df['volatility_20'].dropna()
                percentiles = np.percentile(volatility_series, [25, 75])
                
                if latest['volatility_20'] < percentiles[0]:
                    conditions.append("Low volatility regime (bottom 25% of recent range)")
                elif latest['volatility_20'] > percentiles[1]:
                    conditions.append("High volatility regime (top 25% of recent range)")
            else:
                # Fallback with fixed thresholds if not enough history
                if latest['volatility_20'] < 0.015:  # 1.5% daily volatility
                    conditions.append("Low volatility")
                elif latest['volatility_20'] > 0.04:  # 4% daily volatility
                    conditions.append("High volatility")
        
        # ---------- PRICE ACTION ANALYSIS ----------
        
        # Recent price performance (5-day)
        if len(df) >= 5:
            five_day_change = ((latest['close'] / df.iloc[-5]['close']) - 1) * 100
            logger.debug(f"5-day price change: {five_day_change:.2f}%")
            
            if five_day_change > 10:
                conditions.append(f"Strong bullish move (+{five_day_change:.1f}% in 5 days)")
            elif five_day_change < -10:
                conditions.append(f"Strong bearish move ({five_day_change:.1f}% in 5 days)")
        
        # Volume analysis
        if 'volume' in df.columns:
            # Calculate average volume over last 20 days
            avg_volume = df['volume'].tail(20).mean()
            latest_volume = latest['volume']
            volume_ratio = latest_volume / avg_volume if avg_volume > 0 else 0
            
            logger.debug(f"Latest volume: {latest_volume}, Avg volume: {avg_volume}, Ratio: {volume_ratio:.2f}")
            
            if volume_ratio > 2.0:
                conditions.append(f"Heavy volume ({volume_ratio:.1f}x average)")
            elif volume_ratio < 0.5:
                conditions.append(f"Light volume ({volume_ratio:.1f}x average)")
        
        # ---------- SUPPORT/RESISTANCE ANALYSIS ----------
        
        # Check for price near recent highs/lows
        if len(df) >= 20:
            recent_high = df['high'].tail(20).max()
            recent_low = df['low'].tail(20).min()
            
            high_distance = (recent_high - latest['close']) / latest['close'] * 100
            low_distance = (latest['close'] - recent_low) / latest['close'] * 100
            
            logger.debug(f"Distance from 20-day high: {high_distance:.2f}%, from low: {low_distance:.2f}%")
            
            if high_distance < 1.0:
                conditions.append("Price near 20-day high (potential resistance)")
            elif low_distance < 1.0:
                conditions.append("Price near 20-day low (potential support)")
        
        # If we didn't identify any conditions (unusual but possible)
        if not conditions:
            logger.warning("No specific market conditions identified")
            conditions.append("Neutral market conditions")
        
        # Log all identified conditions
        logger.info(f"Identified {len(conditions)} market conditions: {', '.join(conditions)}")
        
        return conditions

--- .\quant_system\analysis\__init__.py ---


--- .\quant_system\data\connectors.py ---
# quant_system/data/connectors.py
import os
import pandas as pd
import ccxt
import traceback
from datetime import datetime, timedelta
import sys
import time

# Add the project root to the path if needed
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

# Import our logging utilities and cache
from quant_system.utils import get_logger, ErrorHandler
from quant_system.data.cache import DataCache

# Initialize logger for this module
logger = get_logger("data.connectors")

class CryptoDataConnector:
    """Connector for cryptocurrency market data with caching"""
    
    def __init__(self, exchange_id='coinbase', use_cache=True, cache_dir="cache"):
        """Initialize the data connector
        
        Args:
            exchange_id: ID of the exchange to use (e.g., 'coinbase', 'binance')
            use_cache: Whether to use data caching
            cache_dir: Directory for cached data
        """
        logger.info(f"Initializing CryptoDataConnector with exchange: {exchange_id}")
        self.use_cache = use_cache
        
        try:
            self.exchange = getattr(ccxt, exchange_id)({
                'enableRateLimit': True,
            })
            logger.debug(f"Successfully connected to {exchange_id} API")
            
            # Initialize cache if enabled
            if self.use_cache:
                self.cache = DataCache(cache_dir=cache_dir)
                logger.info("Data caching enabled")
            else:
                self.cache = None
                logger.info("Data caching disabled")
                
        except Exception as e:
            logger.error(f"Failed to initialize exchange {exchange_id}: {e}")
            logger.debug(traceback.format_exc())
            raise
    
    def _timeframe_to_milliseconds(self, timeframe):
        """Convert a timeframe string to milliseconds"""
        unit = timeframe[-1]
        value = int(timeframe[:-1])
        
        if unit == 'm':
            return value * 60 * 1000
        elif unit == 'h':
            return value * 60 * 60 * 1000
        elif unit == 'd':
            return value * 24 * 60 * 60 * 1000
        elif unit == 'w':
            return value * 7 * 24 * 60 * 60 * 1000
        else:
            logger.warning(f"Unknown timeframe unit: {unit}, defaulting to days")
            return value * 24 * 60 * 60 * 1000
    
    def _timeframe_to_minutes(self, timeframe):
        """Convert a timeframe string to minutes"""
        unit = timeframe[-1]
        value = int(timeframe[:-1])
        
        if unit == 'm':
            return value
        elif unit == 'h':
            return value * 60
        elif unit == 'd':
            return value * 24 * 60
        elif unit == 'w':
            return value * 7 * 24 * 60
        else:
            logger.warning(f"Unknown timeframe unit: {unit}, defaulting to days")
            return value * 24 * 60
    
    def fetch_extended_history(self, symbol='BTC/USD', timeframe='1d', days=365):
        """
        DEPRECATED: Use fetch_market_data instead.
        This method remains for backward compatibility only.
        """
        logger.warning("fetch_extended_history is deprecated, use fetch_market_data instead")
        
        # Calculate a date range going back 'days' from today
        to_date = datetime.now()
        from_date = to_date - timedelta(days=days)
        
        # Use the unified function
        return self.fetch_market_data(
            symbol=symbol, 
            timeframe=timeframe, 
            from_date=from_date,
            to_date=to_date
        )
    
    def fetch_market_data(self, symbol='BTC/USD', timeframe='1d', limit=None, 
                         from_date=None, to_date=None, add_indicators=False,
                         force_refresh=False, csv_output=None, retry_delay=30):
        """Unified market data fetching function that handles caching, pagination and indicators
        
        Args:
            symbol: Trading pair symbol
            timeframe: Candle timeframe
            limit: Optional number of candles to return (most recent)
            from_date: Optional start date as string 'YYYY-MM-DD' or datetime object
            to_date: Optional end date as string 'YYYY-MM-DD' or datetime object
            add_indicators: Whether to calculate and cache technical indicators
            force_refresh: Whether to force fetch from API ignoring cache
            csv_output: Optional path to save data as CSV
            retry_delay: Delay in seconds before retrying on error
            
        Returns:
            DataFrame with OHLCV data and optional indicators
        """
        market_data = None
        
        # 1. Determine what we need to fetch
        logger.info(f"Fetching market data for {symbol} ({timeframe})")
        
        # Convert date strings to timestamps if provided
        from_timestamp = None
        to_timestamp = None
        
        if from_date is not None:
            if isinstance(from_date, str):
                # Add time component if missing
                if len(from_date) == 10:  # YYYY-MM-DD format
                    from_date = f"{from_date} 00:00:00"
                from_timestamp = self.exchange.parse8601(from_date)
                if from_timestamp is None:
                    logger.error(f"Could not parse from_date: {from_date}")
                    from_timestamp = int((datetime.now() - timedelta(days=365)).timestamp() * 1000)
            elif isinstance(from_date, datetime):
                from_timestamp = int(from_date.timestamp() * 1000)
            else:
                from_timestamp = from_date  # Assume it's already a timestamp
                
        if to_date is not None:
            if isinstance(to_date, str):
                # Add time component if missing
                if len(to_date) == 10:  # YYYY-MM-DD format
                    to_date = f"{to_date} 23:59:59"
                to_timestamp = self.exchange.parse8601(to_date)
                if to_timestamp is None:
                    logger.error(f"Could not parse to_date: {to_date}")
                    to_timestamp = int(datetime.now().timestamp() * 1000)
            elif isinstance(to_date, datetime):
                to_timestamp = int(to_date.timestamp() * 1000)
            else:
                to_timestamp = to_date  # Assume it's already a timestamp
        
        # 2. Check cache if enabled and not forcing refresh
        if self.use_cache and not force_refresh:
            cached_data = self.cache.get_cached_ohlcv(symbol, timeframe, max_age_days=1)
            
            if cached_data is not None and not cached_data.empty:
                logger.info(f"Found cached data for {symbol} ({len(cached_data)} records)")
                
                # If we have date range, filter the cached data
                if from_timestamp or to_timestamp:
                    if from_timestamp:
                        from_dt = datetime.fromtimestamp(from_timestamp / 1000)
                        cached_data = cached_data[cached_data.index >= from_dt]
                    if to_timestamp:
                        to_dt = datetime.fromtimestamp(to_timestamp / 1000)
                        cached_data = cached_data[cached_data.index <= to_dt]
                    
                    logger.debug(f"Filtered cached data to {len(cached_data)} records")
                
                # If we have limit, return only that many recent records
                if limit and len(cached_data) >= limit:
                    market_data = cached_data.iloc[-limit:]
                    logger.info(f"Using {len(market_data)} records from cache (limit={limit})")
                else:
                    # We have complete cached data for the request
                    market_data = cached_data
                
                # Check if we need to update with the latest data
                if market_data is not None:
                    # Check if the latest candle is from the current period
                    latest_candle_time = market_data.index[-1]
                    current_time = datetime.now()
                    
                    # Calculate the timeframe in minutes
                    timeframe_minutes = self._timeframe_to_minutes(timeframe)
                    
                    seconds_since_last_candle = (current_time - latest_candle_time).total_seconds()
                    
                    # If the latest candle is older than one candle period but less than two periods,
                    # we might need a new candle
                    if timeframe_minutes * 60 < seconds_since_last_candle < timeframe_minutes * 60 * 2:
                        logger.info(f"Checking for newer data since {latest_candle_time}")
                        
                        try:
                            new_candles = self.exchange.fetch_ohlcv(symbol, timeframe, limit=2)
                            if new_candles and len(new_candles) > 0:
                                newest_timestamp = new_candles[-1][0]
                                newest_dt = datetime.fromtimestamp(newest_timestamp / 1000)
                                
                                if newest_dt > latest_candle_time:
                                    logger.info(f"Found newer data, updating cache with candle from {newest_dt}")
                                    new_df = pd.DataFrame(new_candles, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume'])
                                    new_df['timestamp'] = pd.to_datetime(new_df['timestamp'], unit='ms')
                                    new_df.set_index('timestamp', inplace=True)
                                    
                                    # Add to cache
                                    updated_cache = pd.concat([cached_data, new_df])
                                    updated_cache = updated_cache[~updated_cache.index.duplicated(keep='last')]
                                    updated_cache = updated_cache.sort_index()
                                    
                                    # Update the cache
                                    self.cache.cache_ohlcv(symbol, timeframe, updated_cache)
                                    
                                    # Update our result
                                    if limit and len(updated_cache) >= limit:
                                        market_data = updated_cache.iloc[-limit:]
                                    else:
                                        market_data = updated_cache
                        except Exception as e:
                            logger.warning(f"Error checking for newer data: {e}")
        
        # 3. If we don't have complete data from cache, fetch from exchange
        if market_data is None or (
            (from_timestamp and market_data.index.min() > datetime.fromtimestamp(from_timestamp / 1000)) or
            (to_timestamp and market_data.index.max() < datetime.fromtimestamp(to_timestamp / 1000))
        ):
            logger.info("Need to fetch data from exchange")
            all_ohlcv = []
            
            # Determine if we need pagination
            need_pagination = from_timestamp is not None or limit is None or limit > 1000
            
            if need_pagination:
                # Use pagination to fetch all data
                logger.info("Using pagination to fetch complete data")
                
                # Start from the earliest timestamp we need
                current_timestamp = from_timestamp
                if current_timestamp is None:
                    # Default to 1 year ago if not specified
                    current_timestamp = int((datetime.now() - timedelta(days=365)).timestamp() * 1000)
                
                # End at the latest timestamp we need
                end_timestamp = to_timestamp
                if end_timestamp is None:
                    end_timestamp = int(datetime.now().timestamp() * 1000)
                
                while current_timestamp < end_timestamp:
                    try:
                        logger.debug(f"Fetching candles from {self.exchange.iso8601(current_timestamp)}")
                        # Most exchanges limit to 1000 candles per request
                        ohlcvs = self.exchange.fetch_ohlcv(symbol, timeframe, since=current_timestamp, limit=1000)
                        
                        if not ohlcvs or len(ohlcvs) == 0:
                            logger.warning(f"No data returned at timestamp {current_timestamp}, stopping pagination")
                            break
                            
                        logger.debug(f"Fetched {len(ohlcvs)} candles")
                        
                        # Filter out any data beyond our end timestamp
                        if to_timestamp:
                            ohlcvs = [candle for candle in ohlcvs if candle[0] <= to_timestamp]
                        
                        # Add to our result set
                        all_ohlcv.extend(ohlcvs)
                        
                        # Update timestamp for next iteration based on last candle
                        if len(ohlcvs) > 0:
                            # Move timestamp forward by 1ms to avoid duplicates
                            current_timestamp = ohlcvs[-1][0] + 1
                        else:
                            # If we got an empty response but didn't break earlier, move forward in time
                            timeframe_ms = self._timeframe_to_milliseconds(timeframe)
                            current_timestamp += timeframe_ms * 100  # Skip ahead 100 candles
                        
                        # If we got fewer candles than expected, we might be at the end
                        if len(ohlcvs) < 1000:  # Most exchanges return max 1000 candles per request
                            logger.debug("Received fewer candles than expected, may have reached the end")
                            # Only break if we actually got some candles, otherwise continue
                            if len(ohlcvs) > 0:
                                break
                    
                    except (ccxt.ExchangeError, ccxt.AuthenticationError,
                           ccxt.ExchangeNotAvailable, ccxt.RequestTimeout) as error:
                        error_msg = f"Error fetching data: {type(error).__name__}, {error.args}"
                        logger.error(error_msg)
                        logger.info(f"Retrying in {retry_delay} seconds...")
                        
                        # Wait before retrying
                        time.sleep(retry_delay)
                    
                    except Exception as e:
                        logger.error(f"Unexpected error fetching paginated data: {e}")
                        logger.debug(traceback.format_exc())
                        break
            else:
                # Simple fetch with limit
                try:
                    logger.info(f"Fetching {limit} most recent candles")
                    ohlcvs = self.exchange.fetch_ohlcv(symbol, timeframe, limit=limit)
                    all_ohlcv = ohlcvs
                except Exception as e:
                    logger.error(f"Error fetching data: {e}")
                    logger.debug(traceback.format_exc())
            
            # Convert to DataFrame
            if all_ohlcv:
                df = pd.DataFrame(all_ohlcv, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume'])
                df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')
                df.set_index('timestamp', inplace=True)
                
                # Remove duplicates that might occur at pagination boundaries
                df = df[~df.index.duplicated(keep='first')]
                df = df.sort_index()
                
                logger.info(f"Successfully fetched {len(df)} records from {df.index[0]} to {df.index[-1]}")
                
                # Merge with cached data if we have any
                if market_data is not None:
                    logger.info("Merging new data with cached data")
                    combined_df = pd.concat([market_data, df])
                    combined_df = combined_df[~combined_df.index.duplicated(keep='last')]  # Keep newer versions
                    combined_df = combined_df.sort_index()
                    
                    # Update the result
                    if limit and len(combined_df) > limit:
                        market_data = combined_df.iloc[-limit:]
                    else:
                        market_data = combined_df
                else:
                    # Use the fetched data directly
                    if limit and len(df) > limit:
                        market_data = df.iloc[-limit:]
                    else:
                        market_data = df
                
                # Update the cache
                if self.use_cache:
                    logger.debug(f"Updating OHLCV cache for {symbol}")
                    self.cache.update_ohlcv_cache(symbol, timeframe, df)
            else:
                # No data fetched, use whatever we had from cache
                logger.warning(f"No data fetched from exchange for {symbol}")
                if market_data is None:
                    logger.warning("No data available at all")
                    market_data = pd.DataFrame()
        
        # 4. If we have data but not indicators, and indicators were requested, calculate them
        if add_indicators and not market_data.empty:
            logger.info(f"Calculating technical indicators for {len(market_data)} candles")
            
            # Import here to avoid circular imports
            from quant_system.features.technical import TechnicalFeatures
            
            # Initialize with the same cache but force recalculation
            tech = TechnicalFeatures(cache=self.cache)
            
            # Force recalculation of indicators for the full dataset
            # First, calculate all indicators without going through cache
            df_indicators = tech._calculate_indicators(market_data)
            
            # Then update the cache with the new indicators
            if self.use_cache:
                logger.info(f"Updating indicators cache with {len(df_indicators)} records")
                self.cache.cache_indicators(symbol, timeframe, df_indicators)
            
            # Use the newly calculated indicators
            market_data = df_indicators
            
            logger.info(f"Added indicators to market data ({len(market_data.columns) - 5} indicators)")
        
        # 5. Export to CSV if requested
        if csv_output and not market_data.empty:
            try:
                logger.info(f"Saving market data to {csv_output}")
                
                # Create directory if it doesn't exist
                os.makedirs(os.path.dirname(os.path.abspath(csv_output)), exist_ok=True)
                
                # Save to CSV
                market_data.to_csv(csv_output)
                logger.info(f"Successfully saved {len(market_data)} records to {csv_output}")
            except Exception as e:
                logger.error(f"Failed to save data to {csv_output}: {e}")
                logger.debug(traceback.format_exc())
        
        return market_data
        
    def fetch_latest_ticker(self, symbol='BTC/USD'):
        """Fetch the latest ticker information for a symbol"""
        logger.info(f"Fetching latest ticker for {symbol}")
        
        try:
            ticker = self.exchange.fetch_ticker(symbol)
            logger.debug(f"Ticker data received for {symbol}")
            
            # Log the important metrics
            logger.info(f"{symbol} last price: {ticker['last']}, 24h change: {ticker.get('percentage', 0):.2f}%")
            
            return ticker
        except Exception as e:
            logger.error(f"Failed to fetch ticker for {symbol}: {e}")
            logger.debug(traceback.format_exc())
            return None
            
    def fetch_order_book(self, symbol='BTC/USD', limit=20):
        """Fetch the current order book for a symbol"""
        logger.info(f"Fetching order book for {symbol} (limit={limit})")
        
        try:
            order_book = self.exchange.fetch_order_book(symbol, limit)
            
            # Log order book stats
            top_bid = order_book['bids'][0][0] if order_book['bids'] else None
            top_ask = order_book['asks'][0][0] if order_book['asks'] else None
            
            if top_bid and top_ask:
                spread = (top_ask - top_bid) / top_bid * 100
                logger.info(f"{symbol} order book - Top bid: {top_bid}, Top ask: {top_ask}, Spread: {spread:.3f}%")
            else:
                logger.warning(f"Incomplete order book data for {symbol}")
            
            return order_book
        except Exception as e:
            logger.error(f"Failed to fetch order book for {symbol}: {e}")
            logger.debug(traceback.format_exc())
            return None

    def fetch_multiple_symbols(self, symbols=['BTC/USD', 'ETH/USD'], timeframe='1d', limit=100, 
                            from_date=None, to_date=None, add_indicators=False):
        """Fetch market data for multiple symbols
        
        Args:
            symbols: List of trading pair symbols
            timeframe: Candle timeframe
            limit: Optional number of candles to return (most recent)
            from_date: Optional start date
            to_date: Optional end date
            add_indicators: Whether to calculate and cache technical indicators
            
        Returns:
            Dictionary of symbol -> DataFrame with market data
        """
        logger.info(f"Fetching market data for {len(symbols)} symbols ({timeframe})")
        
        result = {}
        with ErrorHandler(context="fetching multiple symbols data") as handler:
            for symbol in symbols:
                logger.debug(f"Processing symbol: {symbol}")
                df = self.fetch_market_data(
                    symbol=symbol,
                    timeframe=timeframe,
                    limit=limit,
                    from_date=from_date,
                    to_date=to_date,
                    add_indicators=add_indicators
                )
                
                if not df.empty:
                    result[symbol] = df
                else:
                    logger.warning(f"No data available for {symbol}, skipping")
        
        logger.info(f"Successfully fetched data for {len(result)}/{len(symbols)} symbols")
        return result

--- .\quant_system\data\processors.py ---


--- .\quant_system\data\storage.py ---


--- .\quant_system\data\__init__.py ---


--- .\quant_system\features\correlation.py ---


--- .\quant_system\features\macro.py ---


--- .\quant_system\features\technical.py ---
# quant_system/features/technical.py
import numpy as np
import pandas as pd
import ta
import math
from datetime import datetime, timedelta

from quant_system.utils import get_logger

# Initialize logger
logger = get_logger("features.technical")

class TechnicalFeatures:
    """Generate technical indicators from price data"""
    
    def __init__(self, cache=None):
        """Initialize the technical features calculator
        
        Args:
            cache: Optional DataCache instance for caching indicators
        """
        self.cache = cache
        self.logger = get_logger("features.technical")
    
    def add_indicators(self, df, symbol, timeframe, required_periods=None):
        """Add common technical indicators to a dataframe
        
        Adapts the indicators based on available data length.
        Uses caching if available.
        
        Args:
            df: DataFrame with OHLCV data
            symbol: Trading pair symbol (e.g., 'BTC/USD')
            timeframe: Timeframe (e.g., '1d', '1h')
            required_periods: Optional list of periods to ensure are calculated
            
        Returns:
            DataFrame with technical indicators added
        """
        if df.empty:
            self.logger.warning("Cannot add indicators: Empty dataframe")
            return df
        
        # Try to get cached indicators first
        if self.cache is not None:
            cached_indicators = self.cache.get_cached_indicators(symbol, timeframe)
            if cached_indicators is not None:
                self.logger.info(f"Using cached indicators for {symbol} ({timeframe})")
                # Ensure we have the latest data by updating the last candle
                if not cached_indicators.empty and not df.empty:
                    latest_cached = cached_indicators.index[-1]
                    latest_price = df.index[-1]
                    if latest_cached == latest_price:
                        # Update the latest candle's indicators
                        new_indicators = self._calculate_indicators(df.iloc[-1:], required_periods)
                        cached_indicators.iloc[-1] = new_indicators.iloc[-1]
                        self.cache.cache_indicators(symbol, timeframe, cached_indicators)
                return cached_indicators
        
        # Calculate indicators if not cached or cache is stale
        self.logger.info(f"Calculating indicators for {symbol} ({timeframe})")
        df_indicators = self._calculate_indicators(df, required_periods)
        
        # Cache the results if we have a cache instance
        if self.cache is not None:
            self.cache.cache_indicators(symbol, timeframe, df_indicators)
        
        return df_indicators
    
    def _calculate_indicators(self, df, required_periods=None):
        """Calculate technical indicators for a dataframe
        
        This is the internal method that does the actual calculation.
        The public add_indicators method handles caching.
        
        Args:
            df: DataFrame with OHLCV data
            required_periods: Optional list of periods to ensure are calculated
            
        Returns:
            DataFrame with technical indicators added
        """
        if df.empty:
            self.logger.warning("Cannot calculate indicators: Empty dataframe")
            return df
        
        data_length = len(df)
        self.logger.info(f"Calculating technical indicators for dataframe with {data_length} rows")
        
        # Copy the dataframe to avoid modifying the original
        df_indicators = df.copy()
        
        # Log the date range
        start_date = df_indicators.index[0]
        end_date = df_indicators.index[-1]
        self.logger.debug(f"Data range: {start_date.date()} to {end_date.date()}")
        
        # Determine maximum reasonable periods based on data length
        # Rule of thumb: Don't use MA periods > data_length/2
        max_ma_period = min(200, math.floor(data_length/2))
        
        # If we have very limited data, adjust all periods proportionally
        if data_length < 50:
            self.logger.warning(f"Limited data available ({data_length} rows), scaling indicator periods")
            scale_factor = data_length / 100  # Scale based on ideal 100+ data points
        else:
            scale_factor = 1.0
            
        self.logger.debug(f"Using max MA period of {max_ma_period}, scale factor {scale_factor:.2f}")
        
        # ---------- MOVING AVERAGES ----------
        
        # Add moving averages based on available data
        ma_periods = [5, 10, 20, 50, 200]
        ma_periods = [p for p in ma_periods if p <= max_ma_period]
        
        for period in ma_periods:
            adjusted_period = max(2, int(period * scale_factor))
            self.logger.debug(f"Calculating SMA with period {adjusted_period} (original: {period})")
            
            df_indicators[f'sma_{period}'] = ta.trend.sma_indicator(
                df_indicators['close'], 
                window=adjusted_period
            )
        
        # ---------- MOMENTUM INDICATORS ----------
        
        # RSI (default 14, min 2)
        rsi_period = max(2, int(14 * scale_factor))
        self.logger.debug(f"Calculating RSI with period {rsi_period}")
        df_indicators['rsi_14'] = ta.momentum.rsi(df_indicators['close'], window=rsi_period)
        
        # MACD (default 12/26/9)
        # Adjust if we have limited data
        if data_length >= 30:
            fast_period = int(12 * scale_factor)
            slow_period = int(26 * scale_factor)
            signal_period = int(9 * scale_factor)
            
            # Ensure minimum values
            fast_period = max(2, fast_period)
            slow_period = max(fast_period + 1, slow_period)
            signal_period = max(2, signal_period)
            
            self.logger.debug(f"Calculating MACD with periods {fast_period}/{slow_period}/{signal_period}")
            
            macd = ta.trend.MACD(
                df_indicators['close'], 
                window_fast=fast_period, 
                window_slow=slow_period, 
                window_sign=signal_period
            )
            df_indicators['macd'] = macd.macd()
            df_indicators['macd_signal'] = macd.macd_signal()
            df_indicators['macd_histogram'] = macd.macd_diff()
        else:
            self.logger.warning(f"Insufficient data for MACD calculation (need 30+, have {data_length})")
        
        # ---------- VOLATILITY INDICATORS ----------
        
        # Bollinger Bands (default 20, min 2)
        bb_period = max(2, int(20 * scale_factor))
        self.logger.debug(f"Calculating Bollinger Bands with period {bb_period}")
        
        bollinger = ta.volatility.BollingerBands(
            df_indicators['close'], 
            window=bb_period, 
            window_dev=2
        )
        df_indicators['bb_upper'] = bollinger.bollinger_hband()
        df_indicators['bb_lower'] = bollinger.bollinger_lband()
        df_indicators['bb_middle'] = bollinger.bollinger_mavg()
        df_indicators['bb_width'] = (df_indicators['bb_upper'] - df_indicators['bb_lower']) / df_indicators['bb_middle']
        
        # ATR (default 14, min 2)
        atr_period = max(2, int(14 * scale_factor))
        self.logger.debug(f"Calculating ATR with period {atr_period}")
        
        df_indicators['atr_14'] = ta.volatility.average_true_range(
            df_indicators['high'], 
            df_indicators['low'], 
            df_indicators['close'], 
            window=atr_period
        )
        
        # Volatility (std dev of returns)
        vol_period = max(2, int(20 * scale_factor))
        self.logger.debug(f"Calculating volatility with period {vol_period}")
        
        df_indicators['volatility_20'] = df_indicators['close'].pct_change().rolling(vol_period).std() * np.sqrt(252 / periodicity_factor(df))
        
        # ---------- Z-SCORES & OSCILLATORS ----------
        
        # Z-score of price distance from MA (for short-term MAs only)
        for period in [20, 50]:
            if f'sma_{period}' in df_indicators.columns:
                ma_col = f'sma_{period}'
                z_col = f'z_score_ma_{period}'
                
                # Calculate z-score if we have enough data
                if data_length >= period * 1.5:
                    self.logger.debug(f"Calculating Z-score for {ma_col}")
                    std_period = max(5, int(period / 2))
                    df_indicators[z_col] = (df_indicators['close'] - df_indicators[ma_col]) / df_indicators['close'].rolling(std_period).std()
        
        # Stochastic Oscillator if we have enough data
        if data_length >= 14:
            stoch_period = max(5, int(14 * scale_factor))
            stoch_smooth = max(3, int(3 * scale_factor))
            
            self.logger.debug(f"Calculating Stochastic Oscillator with periods {stoch_period}/{stoch_smooth}")
            
            stoch = ta.momentum.StochasticOscillator(
                df_indicators['high'], 
                df_indicators['low'], 
                df_indicators['close'], 
                window=stoch_period, 
                smooth_window=stoch_smooth
            )
            df_indicators['stoch_k'] = stoch.stoch()
            df_indicators['stoch_d'] = stoch.stoch_signal()
        
        # ---------- TREND INDICATORS ----------
        
        # ADX if we have enough data
        if data_length >= 28:
            adx_period = max(14, int(14 * scale_factor))
            self.logger.debug(f"Calculating ADX with period {adx_period}")
            
            adx = ta.trend.ADXIndicator(
                df_indicators['high'], 
                df_indicators['low'], 
                df_indicators['close'], 
                window=adx_period
            )
            df_indicators['adx'] = adx.adx()
            df_indicators['di_plus'] = adx.adx_pos()
            df_indicators['di_minus'] = adx.adx_neg()
        
        # ---------- CUSTOM INDICATORS ----------
        
        # Calculate some basic stats about the data
        df_indicators['returns_1d'] = df_indicators['close'].pct_change()
        
        # Calculate distance from recent highs/lows if we have enough data
        if data_length >= 20:
            self.logger.debug("Calculating high/low metrics")
            df_indicators['dist_from_20d_high'] = df_indicators['close'] / df_indicators['high'].rolling(20).max() - 1
            df_indicators['dist_from_20d_low'] = df_indicators['close'] / df_indicators['low'].rolling(20).min() - 1
            
            # Calculate days since last N-day high/low
            for period in [20, 50]:
                if data_length >= period:
                    # Rolling max/min with expanding window method
                    rolling_max = df_indicators['high'].rolling(period).max()
                    rolling_min = df_indicators['low'].rolling(period).min()
                    
                    # Initialize counters
                    days_since_high = np.zeros(len(df_indicators))
                    days_since_low = np.zeros(len(df_indicators))
                    
                    # Calculate days since high/low
                    for i in range(period, len(df_indicators)):
                        if df_indicators['high'].iloc[i] >= rolling_max.iloc[i-1]:
                            # New high
                            days_since_high[i] = 0
                        else:
                            # Increment counter
                            days_since_high[i] = days_since_high[i-1] + 1
                            
                        if df_indicators['low'].iloc[i] <= rolling_min.iloc[i-1]:
                            # New low
                            days_since_low[i] = 0
                        else:
                            # Increment counter
                            days_since_low[i] = days_since_low[i-1] + 1
                    
                    df_indicators[f'days_since_{period}d_high'] = days_since_high
                    df_indicators[f'days_since_{period}d_low'] = days_since_low
        
        # Log count of generated indicators
        num_indicators = len(df_indicators.columns) - len(df.columns)
        self.logger.info(f"Added {num_indicators} technical indicators")
        
        return df_indicators

def periodicity_factor(df):
    """Determine the periodicity factor for annualizing volatility
    
    Args:
        df: DataFrame with datetime index
        
    Returns:
        Factor to use for annualizing (252 for daily, 52 for weekly, etc.)
    """
    if len(df) < 2:
        return 252  # Default to daily
        
    # Calculate average time delta between entries
    deltas = []
    for i in range(1, min(len(df), 10)):
        delta = (df.index[i] - df.index[i-1]).total_seconds()
        deltas.append(delta)
    
    avg_seconds = sum(deltas) / len(deltas)
    
    # Determine periodicity
    if avg_seconds < 3600:  # Less than 1 hour
        minutes = avg_seconds / 60
        logger.debug(f"Detected {minutes:.1f} minute data")
        return 252 * 6.5 * (60 / minutes)  # Assuming 6.5 trading hours
    elif avg_seconds < 3600 * 24:  # Less than 1 day
        hours = avg_seconds / 3600
        logger.debug(f"Detected {hours:.1f} hour data")
        return 252 * (24 / hours)
    elif avg_seconds < 3600 * 24 * 7:  # Less than 1 week
        days = avg_seconds / (3600 * 24)
        logger.debug(f"Detected {days:.1f} day data")
        return 252 / days
    elif avg_seconds < 3600 * 24 * 30:  # Less than 1 month
        weeks = avg_seconds / (3600 * 24 * 7)
        logger.debug(f"Detected {weeks:.1f} week data")
        return 52 / weeks
    else:  # Monthly or longer
        months = avg_seconds / (3600 * 24 * 30)
        logger.debug(f"Detected {months:.1f} month data")
        return 12 / months

--- .\quant_system\features\__init__.py ---


--- .\quant_system\interface\api.py ---
# interface/api.py
from fastapi import FastAPI, HTTPException, Query
import pandas as pd
from pydantic import BaseModel
from typing import List, Dict, Any, Optional
import os
import sys
import time
import traceback

# Add the project root to the path
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# Import our system components
from quant_system.data.connectors import CryptoDataConnector
from quant_system.features.technical import TechnicalFeatures
from quant_system.analysis.market_structure import MarketStructureAnalyzer
from quant_system.analysis.backtest import MarketBacktester
from quant_system.analysis.llm_interface import LLMAnalyzer
from quant_system.utils import get_logger, ErrorHandler

# Initialize logger for this module
logger = get_logger("interface.api")

app = FastAPI(
    title="Quant System API",
    description="API for interacting with the Quantitative Trading System",
    version="0.1.0"
)

# Define request and response models
class AnalysisRequest(BaseModel):
    symbol: str = "BTC/USDT"
    timeframe: str = "1d"
    days: int = 365
    include_raw_data: bool = False

class AnalysisResponse(BaseModel):
    symbol: str
    conditions: List[str]
    similar_instances: int
    backtest_stats: Dict[str, float]
    summary: str
    raw_data: Optional[Dict[str, Any]] = None

# Initialize system components
logger.info("Initializing system components")
try:
    data_connector = CryptoDataConnector()
    technical_features = TechnicalFeatures()
    market_analyzer = MarketStructureAnalyzer()
    backtester = MarketBacktester()
    llm_analyzer = LLMAnalyzer(api_key=os.environ.get("LLM_API_KEY"))
    logger.info("System components initialized successfully")
except Exception as e:
    logger.critical(f"Failed to initialize system components: {e}")
    logger.debug(traceback.format_exc())
    raise

@app.get("/")
def read_root():
    logger.debug("Root endpoint accessed")
    return {"message": "Welcome to the Quant System API"}

@app.post("/analyze", response_model=AnalysisResponse)
def analyze_market(request: AnalysisRequest):
    """Run a complete market analysis for a given symbol"""
    request_id = f"req_{int(time.time())}"
    logger.info(f"[{request_id}] Market analysis requested for {request.symbol} on {request.timeframe} timeframe ({request.days} days)")
    
    with ErrorHandler(context=f"market analysis for {request.symbol}") as handler:
        # 1. Fetch data
        logger.debug(f"[{request_id}] Fetching market data")
        market_data = data_connector.fetch_ohlcv(
            symbol=request.symbol, 
            timeframe=request.timeframe, 
            limit=request.days
        )
        
        if market_data.empty:
            logger.warning(f"[{request_id}] No market data found for {request.symbol}")
            raise HTTPException(status_code=404, detail="Could not fetch market data")
        
        logger.debug(f"[{request_id}] Retrieved {len(market_data)} candles from {market_data.index[0]} to {market_data.index[-1]}")
        
        # 2. Generate technical features
        logger.debug(f"[{request_id}] Generating technical indicators")
        df_indicators = technical_features.add_indicators(market_data)
        
        # 3. Identify current market conditions
        logger.debug(f"[{request_id}] Identifying market conditions")
        conditions = market_analyzer.identify_conditions(df_indicators)
        logger.info(f"[{request_id}] Identified conditions: {', '.join(conditions)}")
        
        # 4. Find similar historical conditions and analyze performance
        logger.debug(f"[{request_id}] Finding similar historical conditions")
        similar_dates = backtester.find_similar_conditions(df_indicators, conditions)
        logger.info(f"[{request_id}] Found {len(similar_dates)} similar historical instances")
        
        # 5. Calculate forward returns from similar conditions
        logger.debug(f"[{request_id}] Calculating historical forward returns")
        results_df, stats = backtester.analyze_forward_returns(df_indicators, similar_dates)
        
        # 6. Generate market summary with LLM
        logger.debug(f"[{request_id}] Generating market summary with LLM")
        summary = llm_analyzer.generate_market_summary(df_indicators, conditions, stats)
        
        # 7. Prepare response
        logger.debug(f"[{request_id}] Preparing API response")
        response = {
            "symbol": request.symbol,
            "conditions": conditions,
            "similar_instances": len(similar_dates),
            "backtest_stats": stats,
            "summary": summary,
            "raw_data": None
        }
        
        # Include raw data if requested
        if request.include_raw_data:
            logger.debug(f"[{request_id}] Including raw data in response")
            response["raw_data"] = {
                "recent_data": df_indicators.tail(20).to_dict(),
                "similar_instances": results_df.to_dict() if not results_df.empty else {}
            }
        
        logger.info(f"[{request_id}] Analysis completed successfully")
        return response

@app.get("/symbols")
def get_available_symbols():
    """Get available trading pairs from the exchange"""
    logger.info("Symbol list requested")
    with ErrorHandler(context="fetching symbols list") as handler:
        exchange = data_connector.exchange
        markets = exchange.load_markets()
        symbols = list(markets.keys())
        logger.info(f"Retrieved {len(symbols)} symbols")
        return {"symbols": symbols}

@app.get("/timeframes")
def get_available_timeframes():
    """Get available timeframes from the exchange"""
    logger.info("Timeframes list requested")
    with ErrorHandler(context="fetching timeframes") as handler:
        exchange = data_connector.exchange
        timeframes = exchange.timeframes
        logger.info(f"Retrieved {len(timeframes)} timeframes")
        return {"timeframes": timeframes}

@app.get("/conditions")
def get_current_conditions(
    symbol: str = Query("BTC/USDT", description="Trading pair symbol"),
    timeframe: str = Query("1d", description="Timeframe for analysis")
):
    """Get current market conditions for a symbol"""
    logger.info(f"Current conditions requested for {symbol} on {timeframe} timeframe")
    with ErrorHandler(context=f"analyzing conditions for {symbol}") as handler:
        # Fetch last 50 days of data
        logger.debug(f"Fetching data for {symbol}")
        market_data = data_connector.fetch_ohlcv(symbol, timeframe, limit=50)
        
        if market_data.empty:
            logger.warning(f"No market data found for {symbol}")
            raise HTTPException(status_code=404, detail="Could not fetch market data")
        
        # Generate indicators
        logger.debug(f"Generating indicators for {symbol}")
        df_indicators = technical_features.add_indicators(market_data)
        
        # Identify conditions
        logger.debug(f"Identifying conditions for {symbol}")
        conditions = market_analyzer.identify_conditions(df_indicators)
        logger.info(f"Identified conditions for {symbol}: {', '.join(conditions)}")
        
        return {
            "symbol": symbol,
            "timeframe": timeframe,
            "conditions": conditions,
            "last_price": float(market_data.iloc[-1]['close']),
            "last_update": market_data.index[-1].isoformat()
        }

@app.get("/backtest")
def backtest_condition(
    condition: List[str] = Query(..., description="Market conditions to backtest"),
    symbol: str = Query("BTC/USDT", description="Trading pair symbol"),
    timeframe: str = Query("1d", description="Timeframe for analysis"),
    days: int = Query(365, description="Days of historical data to analyze")
):
    """Backtest specific market conditions"""
    condition_str = ", ".join(condition)
    logger.info(f"Backtest requested for conditions [{condition_str}] on {symbol} ({timeframe}, {days} days)")
    
    with ErrorHandler(context=f"backtesting {condition_str} on {symbol}") as handler:
        # Fetch data
        logger.debug(f"Fetching {days} days of data for {symbol}")
        market_data = data_connector.fetch_ohlcv(symbol, timeframe, limit=days)
        
        if market_data.empty:
            logger.warning(f"No market data found for {symbol}")
            raise HTTPException(status_code=404, detail="Could not fetch market data")
        
        # Generate indicators
        logger.debug(f"Generating indicators for {symbol}")
        df_indicators = technical_features.add_indicators(market_data)
        
        # Find similar dates
        logger.debug(f"Finding instances of conditions: {condition_str}")
        similar_dates = backtester.find_similar_conditions(df_indicators, condition)
        
        if not similar_dates:
            logger.info(f"No similar conditions found for {condition_str}")
            return {
                "symbol": symbol,
                "conditions": condition,
                "message": "No similar conditions found in the historical data"
            }
        
        # Calculate forward returns
        logger.debug(f"Calculating returns for {len(similar_dates)} similar instances")
        results_df, stats = backtester.analyze_forward_returns(df_indicators, similar_dates)
        
        # Prepare simplified results
        simplified_results = []
        if not results_df.empty:
            for _, row in results_df.iterrows():
                simplified_results.append({
                    "date": row["date"].isoformat(),
                    "conditions": row["conditions"],
                    "returns": {
                        k: float(v) for k, v in row.items() 
                        if k not in ["date", "conditions"] and not pd.isna(v)
                    }
                })
        
        logger.info(f"Backtest completed with {len(similar_dates)} instances")
        return {
            "symbol": symbol,
            "conditions": condition,
            "instances": len(similar_dates),
            "stats": stats,
            "examples": simplified_results[:10]  # Limit to first 10 examples
        }

# Run with: uvicorn interface.api:app --reload
if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)

--- .\quant_system\interface\cli.py ---
# interface/cli.py
import argparse
import json
import os
import sys
import traceback
from typing import List, Dict, Any
import pandas as pd
import time

# Add the project root to the path
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# Import our system components
from quant_system.data.connectors import CryptoDataConnector
from quant_system.features.technical import TechnicalFeatures
from quant_system.analysis.market_structure import MarketStructureAnalyzer
from quant_system.analysis.backtest import MarketBacktester
from quant_system.analysis.llm_interface import LLMAnalyzer
from quant_system.utils import get_logger, ErrorHandler, set_log_level, DEBUG, INFO

# Initialize logger for this module
logger = get_logger("interface.cli")

def pretty_print_json(data):
    """Print JSON data in a human-readable format"""
    print(json.dumps(data, indent=2))

class QuantSystemCLI:
    """Command-line interface for the Quant System"""
    
    def __init__(self):
        logger.debug("Initializing CLI interface")
        try:
            self.data_connector = CryptoDataConnector()
            self.technical_features = TechnicalFeatures(cache=self.data_connector.cache)
            self.market_analyzer = MarketStructureAnalyzer()
            self.backtester = MarketBacktester()
            self.llm = LLMAnalyzer(api_key=os.environ.get("LLM_API_KEY"))
            logger.debug("CLI system components initialized successfully")
        except Exception as e:
            logger.error(f"Failed to initialize CLI system components: {e}")
            logger.debug(traceback.format_exc())
            raise
    
    def analyze(self, symbol: str, timeframe: str, days: int):
        """Run a complete market analysis"""
        print(f"Analyzing {symbol} on {timeframe} timeframe using {days} days of data...\n")
        logger.info(f"Starting analysis of {symbol} on {timeframe} timeframe ({days} days)")
        
        with ErrorHandler(context=f"market analysis for {symbol}") as handler:
            # 1. Fetch data
            logger.debug(f"Fetching market data for {symbol}")
            market_data = self.data_connector.fetch_market_data(
                symbol=symbol, 
                timeframe=timeframe, 
                limit=days
            )
            
            if market_data.empty:
                error_msg = f"Could not fetch market data for {symbol}"
                logger.error(error_msg)
                print(f"Error: {error_msg}")
                return 1
            
            logger.info(f"Retrieved {len(market_data)} candles from {market_data.index[0]} to {market_data.index[-1]}")
            
            # 2. Generate technical features
            logger.debug(f"Generating technical indicators for {symbol}")
            df_indicators = self.technical_features.add_indicators(market_data, symbol, timeframe)
            
            # 3. Identify current market conditions
            logger.debug(f"Identifying market conditions for {symbol}")
            conditions = self.market_analyzer.identify_conditions(df_indicators)
            logger.info(f"Identified conditions for {symbol}: {', '.join(conditions)}")
            
            print("Current Market Conditions:")
            for i, condition in enumerate(conditions, 1):
                print(f"  {i}. {condition}")
            print()
            
            # 4. Find similar historical conditions and analyze performance
            logger.debug(f"Finding similar historical conditions for {symbol}")
            similar_dates = self.backtester.find_similar_conditions(df_indicators, conditions)
            
            if not similar_dates:
                logger.info(f"No similar historical conditions found for {symbol}")
                print("No similar historical conditions found in the specified period.")
                return 0
            
            logger.info(f"Found {len(similar_dates)} similar historical instances for {symbol}")
            print(f"Found {len(similar_dates)} similar historical instances")
            print("Recent examples:")
            for date, matched_conditions in similar_dates[-3:]:
                print(f"  {date.date()}: {', '.join(matched_conditions)}")
            print()
            
            # 5. Calculate forward returns from similar conditions
            logger.debug(f"Calculating forward returns for {symbol}")
            results_df, stats = self.backtester.analyze_forward_returns(df_indicators, similar_dates)
            
            print("Historical Forward Returns (after similar conditions):")
            for period, value in stats.items():
                if 'mean' in period:
                    days = period.split('d_')[0]
                    logger.info(f"{days}-day forward return (avg): {value:.2f}%")
                    print(f"  {days}-day forward return (avg): {value:.2f}%")
                if 'positive_pct' in period:
                    days = period.split('d_')[0]
                    logger.info(f"{days}-day win rate: {value:.2f}%")
                    print(f"  {days}-day win rate: {value:.2f}%")
            print()
            
            # 6. Generate market summary with LLM
            if os.environ.get("LLM_API_KEY"):
                logger.debug(f"Generating LLM market analysis for {symbol}")
                print("Generating market analysis with LLM...")
                summary = self.llm.generate_market_summary(df_indicators, conditions, stats)
                logger.info(f"LLM analysis generated for {symbol}")
                print("\nLLM Analysis:")
                print(summary)
            else:
                logger.warning("LLM analysis skipped (no API key provided)")
                print("\nLLM Analysis skipped (no API key provided)")
        
        logger.info(f"Analysis of {symbol} completed successfully")
        return 0
    
    def list_symbols(self):
        """List available trading pairs"""
        logger.info("Listing available trading symbols")
        with ErrorHandler(context="fetching symbol list") as handler:
            exchange = self.data_connector.exchange
            markets = exchange.load_markets()
            symbols = list(markets.keys())
            
            logger.info(f"Retrieved {len(symbols)} symbols from {exchange.name}")
            print(f"Available symbols on {exchange.name}:")
            
            # Group by base currency
            by_base = {}
            for symbol in symbols:
                parts = symbol.split('/')
                if len(parts) == 2:
                    base = parts[0]
                    if base not in by_base:
                        by_base[base] = []
                    by_base[base].append(symbol)
            
            # Print major currencies first
            major_currencies = ['BTC', 'ETH', 'USDT', 'USDC', 'BNB', 'SOL']
            for currency in major_currencies:
                if currency in by_base:
                    pairs = by_base[currency]
                    logger.debug(f"Displaying {currency} pairs ({len(pairs)} total)")
                    print(f"  {currency}: {', '.join(pairs[:5])}{'...' if len(pairs) > 5 else ''}")
                    del by_base[currency]
            
            # Print remaining currencies
            for currency, pairs in list(by_base.items())[:10]:  # Limit to avoid too much output
                print(f"  {currency}: {', '.join(pairs[:5])}{'...' if len(pairs) > 5 else ''}")
            
            if len(by_base) > 10:
                logger.debug(f"Omitted {len(by_base) - 10} base currencies from display")
                print(f"  ... and {len(by_base) - 10} more base currencies")
            
            logger.info("Symbol list displayed successfully")
            return 0
    
    def list_timeframes(self):
        """List available timeframes"""
        logger.info("Listing available timeframes")
        with ErrorHandler(context="fetching timeframes") as handler:
            exchange = self.data_connector.exchange
            timeframes = exchange.timeframes
            
            logger.info(f"Retrieved {len(timeframes)} timeframes from {exchange.name}")
            print(f"Available timeframes on {exchange.name}:")
            for tf, description in timeframes.items():
                print(f"  {tf}: {description}")
            
            logger.info("Timeframes displayed successfully")
            return 0
    
    def backtest(self, conditions: List[str], symbol: str, timeframe: str, days: int):
        """Backtest specific market conditions"""
        condition_str = ", ".join(conditions)
        print(f"Backtesting conditions: {condition_str}")
        print(f"Symbol: {symbol}, Timeframe: {timeframe}, Data period: {days} days\n")
        
        logger.info(f"Starting backtest for conditions [{condition_str}] on {symbol} ({timeframe}, {days} days)")
        
        with ErrorHandler(context=f"backtesting {condition_str} on {symbol}") as handler:
            # 1. Fetch data
            logger.debug(f"Fetching {days} days of data for {symbol}")
            market_data = self.data_connector.fetch_market_data(
                symbol=symbol,
                timeframe=timeframe,
                limit=days
            )
            
            if market_data.empty:
                error_msg = f"Could not fetch market data for {symbol}"
                logger.error(error_msg)
                print(f"Error: {error_msg}")
                return 1
            
            logger.info(f"Retrieved {len(market_data)} candles for {symbol}")
            
            # 2. Generate technical features
            logger.debug(f"Generating technical indicators for {symbol}")
            df_indicators = self.technical_features.add_indicators(market_data, symbol, timeframe)
            
            # 3. Find similar dates
            logger.debug(f"Finding instances of conditions: {condition_str}")
            similar_dates = self.backtester.find_similar_conditions(df_indicators, conditions)
            
            if not similar_dates:
                logger.info(f"No instances of conditions [{condition_str}] found in historical data")
                print("No instances of these conditions found in the historical data.")
                return 0
            
            logger.info(f"Found {len(similar_dates)} instances of conditions [{condition_str}]")
            print(f"Found {len(similar_dates)} instances of specified conditions")
            
            # 4. Calculate forward returns
            logger.debug(f"Calculating returns for {len(similar_dates)} similar instances")
            results_df, stats = self.backtester.analyze_forward_returns(df_indicators, similar_dates)
            
            print("\nHistorical Forward Returns:")
            for period, value in stats.items():
                if 'mean' in period:
                    days = period.split('d_')[0]
                    logger.info(f"{days}-day forward return (avg): {value:.2f}%")
                    print(f"  {days}-day forward return (avg): {value:.2f}%")
                if 'positive_pct' in period:
                    days = period.split('d_')[0]
                    logger.info(f"{days}-day win rate: {value:.2f}%")
                    print(f"  {days}-day win rate: {value:.2f}%")
            
            if not results_df.empty:
                logger.debug("Displaying most recent occurrences")
                print("\nMost Recent Occurrences:")
                recent = results_df.sort_values('date', ascending=False).head(5)
                for _, row in recent.iterrows():
                    date = row['date'].date()
                    conditions_str = ', '.join(row['conditions'])
                    returns = [f"{days}d: {row[f'{days}d_return']:.2f}%" 
                               for days in [1, 5, 10, 20] 
                               if f"{days}d_return" in row and not pd.isna(row[f"{days}d_return"])]
                    
                    print(f"  {date} - {conditions_str}")
                    print(f"    Returns: {', '.join(returns)}")
            
            logger.info(f"Backtest for conditions [{condition_str}] completed successfully")
            return 0
            
    def fetch_history(self, symbol: str, timeframe: str, from_date: str, to_date: str = None, 
                       output: str = None, add_indicators: bool = False):
        """Fetch complete historical data
        
        Args:
            symbol: Trading pair to fetch
            timeframe: Timeframe to fetch
            from_date: Start date in YYYY-MM-DD format
            to_date: End date in YYYY-MM-DD format (defaults to current date)
            output: Output file path for CSV export
            add_indicators: Whether to calculate and cache technical indicators
        """
        print(f"Fetching complete historical data for {symbol}")
        print(f"Timeframe: {timeframe}")
        print(f"Period: {from_date} to {to_date or 'now'}")
        if add_indicators:
            print("Technical indicators will be calculated\n")
        else:
            print()
        
        logger.info(f"Starting historical data fetch for {symbol} ({timeframe}) from {from_date} to {to_date or 'now'}")
        
        with ErrorHandler(context=f"fetching historical data for {symbol}") as handler:
            # Generate output filename if not provided
            final_output = output
            if output is None and add_indicators:
                # We don't need to specify the output file for indicators
                # The cache_indicators method will save in cache/indicators/ directory
                final_output = None
                logger.info("Indicators will be saved to cache/indicators/ directory")
            elif output is None:
                # Create default filename for OHLCV data
                final_output = f"cache/{symbol.replace('/', '_')}_{timeframe}_ohlcv.csv"
                logger.info(f"Using default output filename: {final_output}")
            
            # Fetch data with the unified function
            market_data = self.data_connector.fetch_market_data(
                symbol=symbol,
                timeframe=timeframe,
                from_date=from_date,
                to_date=to_date,
                add_indicators=add_indicators,
                csv_output=final_output if not add_indicators else None  # Only pass output path for OHLCV data
            )
            
            if market_data.empty:
                error_msg = f"Could not fetch historical data for {symbol}"
                logger.error(error_msg)
                print(f"Error: {error_msg}")
                return 1
            
            # Print data statistics
            print(f"Successfully retrieved {len(market_data)} candles")
            print(f"Time range: {market_data.index[0]} to {market_data.index[-1]}")
            
            if add_indicators:
                indicator_count = len(market_data.columns) - 5  # Subtract OHLCV columns
                print(f"Generated {indicator_count} technical indicators")
                indicator_path = f"cache/indicators/{symbol.replace('/', '_')}_{timeframe}_indicators.csv"
                print(f"\nIndicators saved to {indicator_path}")
            else:
                print(f"\nData successfully saved to {final_output}")
            
            # Print sample of the data
            print("\nData Sample (latest 5 candles):")
            pd.set_option('display.precision', 2)
            pd.set_option('display.max_columns', 10 if add_indicators else 5)  # Limit columns for readability
            pd.set_option('display.width', 120)
            print(market_data.tail(5).to_string())
            
            logger.info(f"Historical data fetch completed successfully for {symbol} ({len(market_data)} records)")
            return 0

def main():
    """Main entry point for the CLI"""
    parser = argparse.ArgumentParser(description="Quant System CLI")
    
    # Add global debug flag that applies to all commands
    parser.add_argument("--debug", action="store_true", help="Enable debug logging")
    
    subparsers = parser.add_subparsers(dest="command", help="Command to run")
    
    # Analyze command
    analyze_parser = subparsers.add_parser("analyze", help="Analyze a market")
    analyze_parser.add_argument("--symbol", "-s", default="BTC/USDT", help="Trading pair to analyze")
    analyze_parser.add_argument("--timeframe", "-t", default="1d", help="Analysis timeframe")
    analyze_parser.add_argument("--days", "-d", type=int, default=365, help="Days of historical data")
    
    # List symbols command
    list_symbols_parser = subparsers.add_parser("symbols", help="List available trading pairs")
    
    # List timeframes command
    list_timeframes_parser = subparsers.add_parser("timeframes", help="List available timeframes")
    
    # Backtest command
    backtest_parser = subparsers.add_parser("backtest", help="Backtest market conditions")
    backtest_parser.add_argument("--conditions", "-c", required=True, nargs="+", help="Market conditions to backtest")
    backtest_parser.add_argument("--symbol", "-s", default="BTC/USDT", help="Trading pair to analyze")
    backtest_parser.add_argument("--timeframe", "-t", default="1d", help="Analysis timeframe")
    backtest_parser.add_argument("--days", "-d", type=int, default=365, help="Days of historical data")
    
    # Fetch historical data command
    history_parser = subparsers.add_parser("history", help="Fetch complete historical data")
    history_parser.add_argument("--symbol", "-s", default="BTC/USDT", help="Trading pair to fetch")
    history_parser.add_argument("--timeframe", "-t", default="1d", help="Data timeframe")
    history_parser.add_argument("--from", dest="from_date", required=True, help="Start date (YYYY-MM-DD)")
    history_parser.add_argument("--to", dest="to_date", help="End date (YYYY-MM-DD), defaults to now")
    history_parser.add_argument("--output", "-o", help="Output file path (CSV)")
    history_parser.add_argument("--indicators", "-i", action="store_true", help="Calculate and cache technical indicators")
    
    # Ensure we don't error on empty args
    if len(sys.argv) <= 1:
        parser.print_help()
        return 0
    
    # Parse the arguments
    try:
        args = parser.parse_args()
    except SystemExit as e:
        # If argument parsing fails, show help and exit with error code
        parser.print_help()
        return e.code
    
    # Set debug logging if requested
    if args.debug:
        set_log_level(DEBUG)
        logger.debug("Debug logging enabled in CLI")
    
    logger.info(f"Starting CLI with command: {args.command or 'help'}")
    
    try:
        cli = QuantSystemCLI()
        
        # Execute the command
        if args.command == "analyze":
            return cli.analyze(args.symbol, args.timeframe, args.days)
        elif args.command == "symbols":
            return cli.list_symbols()
        elif args.command == "timeframes":
            return cli.list_timeframes()
        elif args.command == "backtest":
            return cli.backtest(args.conditions, args.symbol, args.timeframe, args.days)
        elif args.command == "history":
            return cli.fetch_history(args.symbol, args.timeframe, args.from_date, args.to_date, args.output, args.indicators)
        else:
            logger.info("No command specified, showing help")
            parser.print_help()
            return 0
    except Exception as e:
        logger.critical(f"Unhandled exception in CLI: {str(e)}")
        logger.debug(traceback.format_exc())
        print(f"Error: {str(e)}")
        return 1

if __name__ == "__main__":
    sys.exit(main())

--- .\quant_system\interface\__init__.py ---


--- .\quant_system\models\evaluation.py ---


--- .\quant_system\models\prediction.py ---


--- .\quant_system\models\__init__.py ---


